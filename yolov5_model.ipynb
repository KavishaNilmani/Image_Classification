{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec793b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13917d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone YOLOv5 and install dependencies\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt # install dependencies\n",
    "%pip install -q roboflow\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e027f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"D:\\OneDrive - Lowcode Minds Technology Pvt Ltd\\Desktop\\image_classifier_site\\dataset\\train\"\n",
    "os.environ[\"DATASET_DIRECTORY\"] = dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8247e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 1 --data \"D:/OneDrive - Lowcode Minds Technology Pvt Ltd/Desktop/image_classifier_site/dataset/dataset.yaml\" --weights yolov5s.pt --cache --single-cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4989e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "# Launch after you have started training\n",
    "# logs save in the folder \"runs\"\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8897f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:/OneDrive - Lowcode Minds Technology Pvt Ltd/Desktop/image_classifier_site/yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.1 --source {dataset.location}/val/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display inference on ALL test images\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for imageName in glob.glob('yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
    "    display(Image(filename=imageName))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './runs/train/exp/weights/best.pt'\n",
    "\n",
    "import os\n",
    "if os.path.exists(model_path):\n",
    "    print(\"✅ Model found at:\", model_path)\n",
    "else:\n",
    "    print(\"❌ Model not found at:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2221622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
